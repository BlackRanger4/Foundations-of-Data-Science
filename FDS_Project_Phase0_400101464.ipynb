{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font face=\"Times New Roman\" size=5>\n",
        "<div dir=rtl align=\"center\">\n",
        "<font face=\"Times New Roman\" size=5>\n",
        "In The Name of God\n",
        "</font>\n",
        "<br>\n",
        "<img src=\"https://logoyar.com/content/wp-content/uploads/2021/04/sharif-university-logo.png\" alt=\"University Logo\" width=\"150\" height=\"150\">\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=4 align=center>\n",
        "Sharif University of Technology - Department of Electrical Engineering\n",
        "</font>\n",
        "<br>\n",
        "<font color=\"#008080\" size=6>\n",
        "Foundations of Data Science\n",
        "</font>\n",
        "<hr/>\n",
        "<font color=\"#800080\" size=5>\n",
        "Phase 0 Report: Web Scraping and Data Processing for AI Paper Retrieval\n",
        "<br>\n",
        "</font>\n",
        "<font size=5>\n",
        "Instructor: Dr. Khalaj\n",
        "<br>\n",
        "</font>\n",
        "<font size=4>\n",
        "Fall 2024\n",
        "<br>\n",
        "<font face=\"Times New Roman\" size=4>\n",
        "Ali Sadeghiyan 400101464\n",
        "</font>\n",
        "\n",
        "</div></font>"
      ],
      "metadata": {
        "id": "2t-KEVWQQzVx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Introduction**  \n",
        "The objective of this project is to develop an AI assistant that helps users find relevant **scholarly papers**. The dataset provided (DBLP) contains papers up to 2017, but to enhance the assistant’s capability, we need to **retrieve more recent papers** from **online sources**. This phase focuses on **web scraping**, where we extract relevant research papers from **Semantic Scholar** for five key topics in artificial intelligence:  \n",
        "\n",
        "- **Foundation Models**  \n",
        "- **Generative Models**  \n",
        "- **Large Language Models (LLM)**  \n",
        "- **Vision-Language Models (VLM)**  \n",
        "- **Diffusion Models**  \n",
        "\n",
        "This report details the **crawling methodology**, **data storage format**, and **processing steps**, as well as the **challenges encountered** and **next steps**.  \n",
        "\n",
        "## **2. Data Collection Methodology**  \n",
        "To ensure efficient data retrieval, we designed an automated **scraping pipeline** using the **Semantic Scholar API**. The main objectives were:  \n",
        "\n",
        "1. **Querying Papers:** The script searches for each topic and retrieves **relevant papers published after 2017**.  \n",
        "2. **Sorting and Filtering:** The results are sorted by **relevance**, and only specific fields are extracted, including **title, abstract, authors, and citation count**.  \n",
        "3. **Pagination Handling:** Since the API returns a limited number of results per request, we implemented a mechanism to **iteratively fetch additional papers** until the required number is reached.  \n",
        "4. **Rate Limiting:** To avoid being blocked due to excessive requests, the script monitors the API’s response and implements a **retry strategy** when necessary.  \n",
        "5. **Error Handling:** The script ensures robustness by handling network failures, missing data fields, and unexpected API responses.  \n",
        "\n",
        "The scraping process was conducted for three year ranges: **2017-2020**,**2021-2023** and **2024-2025**, with a goal of collecting at least **2000 research papers per topic**.  \n",
        "\n",
        "## **3. Data Storage and Structure**  \n",
        "The extracted data was structured and saved in **JSON format**, ensuring easy access and further processing. Each topic has a separate file, and the format is as follows:  \n",
        "\n",
        "```json\n",
        "[\n",
        "  {\n",
        "    \"year_range\": \"2017-2023\",\n",
        "    \"papers\": [\n",
        "      {\n",
        "        \"title\": \"Example Paper Title\",\n",
        "        \"abstract\": \"This paper explores...\",\n",
        "        \"authors\": \"Author1, Author2\",\n",
        "        \"citations\": 23\n",
        "      }\n",
        "    ]\n",
        "  }\n",
        "]\n",
        "```\n",
        "This structured approach allows easy merging, filtering, and querying for further analysis.  \n",
        "\n",
        "## **4. Merging and Processing Data**  \n",
        "After collecting the research papers, additional **data processing steps** were implemented:  \n",
        "\n",
        "- **Merging Paper Lists:** Since each topic had papers from different year ranges, the script combines them into a single list for **consistency**.  \n",
        "- **Handling Missing Data:** Some papers lack abstracts or author names. Default placeholders were used to ensure that all records remain structured.  \n",
        "- **Removing Duplicates:** If a paper appeared multiple times across year ranges, it was identified and removed to maintain dataset integrity.  \n",
        "\n",
        "A separate script was developed to **load, clean, and merge** these datasets, ensuring that all data is correctly formatted for later use.  \n",
        "\n",
        "## **5. Data Verification**  \n",
        "To confirm the quality of the scraped data, we implemented a **randomized paper selection** method. This allows us to randomly sample and inspect papers from each dataset. The script ensures that:  \n",
        "\n",
        "- The retrieved papers contain **meaningful abstracts and author information**.  \n",
        "- The **citations field** is correctly extracted and stored.  \n",
        "- There are no **empty or incomplete entries** in the dataset.  \n",
        "\n",
        "## **6. Challenges and Considerations**  \n",
        "While implementing the scraping and data processing pipeline, several challenges arose:  \n",
        "\n",
        "- **API Rate Limits:** The Semantic Scholar API enforces request restrictions, requiring **delays and retries** to avoid getting blocked.  \n",
        "- **Inconsistent Metadata:** Some papers lack essential details like abstracts or author names, requiring **data cleaning techniques**.  \n",
        "- **Handling Large-Scale Data:** Collecting and processing thousands of papers required **efficient file handling** to avoid performance issues.  \n",
        "- **Filtering Irrelevant Results:** Ensuring that retrieved papers **strictly belong to the specified topics** was a challenge, as some results contained loosely related content.  \n",
        "\n",
        "## **7. Conclusion**  \n",
        "This phase successfully established a **structured web scraping and data processing framework** for retrieving AI research papers. The pipeline ensures **efficient data retrieval, storage, and cleaning**, forming the foundation for the **next phase of AI assistant development**.  \n",
        "\n",
        "The collected data will now be used for **training and evaluation**, paving the way for building a system that can effectively assist researchers in finding relevant academic papers.  \n",
        "\n"
      ],
      "metadata": {
        "id": "YsaszttuQrVx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnSjMLz9-xsg"
      },
      "source": [
        "# Crawling:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import time\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Function to scrape research papers from the Semantic Scholar API\n",
        "def scrape_semantic_scholar_api(topic, year_range, limit=1000):\n",
        "    \"\"\"\n",
        "    Fetches research papers related to the specified topic from the Semantic Scholar API.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The research topic to search for.\n",
        "        year_range (str): The range of years to filter papers (e.g., \"2017-2023\").\n",
        "        limit (int): The maximum number of papers to retrieve.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of dictionaries containing paper details (title, abstract, authors, citations).\n",
        "    \"\"\"\n",
        "    api_url = \"https://api.semanticscholar.org/graph/v1/paper/search\"\n",
        "    collected_papers = []\n",
        "    current_offset = 0\n",
        "\n",
        "    while len(collected_papers) < limit:\n",
        "        if current_offset % 300 == 0:\n",
        "            print(f\"Scraped: {len(collected_papers)}/{limit}\")\n",
        "\n",
        "        # Construct query parameters for the API request\n",
        "        query_params = {\n",
        "            \"query\": topic,\n",
        "            \"fields\": \"title,abstract,authors,citationCount\",\n",
        "            \"offset\": current_offset,\n",
        "            \"limit\": 100,\n",
        "            \"year\": year_range,\n",
        "            \"sort\": \"relevance\"\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(api_url, params=query_params)\n",
        "\n",
        "            # Handle rate limits\n",
        "            if response.status_code == 429:\n",
        "                print(\"Hit rate limit. Pausing for 10 seconds...\")\n",
        "                time.sleep(10)\n",
        "                continue\n",
        "\n",
        "            # Handle unexpected errors\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Error: Received {response.status_code}. Message: {response.json()}.\")\n",
        "                break\n",
        "\n",
        "            response_data = response.json()\n",
        "            paper_list = response_data.get(\"data\", [])\n",
        "\n",
        "            # Stop if no more papers are found\n",
        "            if not paper_list:\n",
        "                print(\"No additional papers found.\")\n",
        "                break\n",
        "\n",
        "            # Process each paper and extract relevant information\n",
        "            for paper in paper_list:\n",
        "                try:\n",
        "                    collected_papers.append({\n",
        "                        'title': paper.get(\"title\", \"No Title\"),\n",
        "                        'abstract': paper.get(\"abstract\", \"No Abstract\"),\n",
        "                        'authors': \", \".join([author.get(\"name\", \"Unknown\") for author in paper.get(\"authors\", [])]),\n",
        "                        'citations': paper.get(\"citationCount\", 0)\n",
        "                    })\n",
        "\n",
        "                    # Stop if the required limit is reached\n",
        "                    if len(collected_papers) >= limit:\n",
        "                        break\n",
        "\n",
        "                except Exception as parse_error:\n",
        "                    print(f\"Error processing paper data: {parse_error}\")\n",
        "\n",
        "            # Move to the next set of results\n",
        "            current_offset += 100\n",
        "            time.sleep(5)  # Avoid making too many requests too quickly\n",
        "\n",
        "        except requests.exceptions.RequestException as request_error:\n",
        "            print(f\"Network or request error: {request_error}. Retrying in 10 seconds...\")\n",
        "            time.sleep(10)\n",
        "\n",
        "    return collected_papers\n",
        "\n",
        "# Function to save scraped papers to a JSON file\n",
        "def save_to_json_file(topic, all_year_data):\n",
        "    \"\"\"\n",
        "    Saves the collected paper data to a JSON file.\n",
        "\n",
        "    Parameters:\n",
        "        topic (str): The research topic.\n",
        "        all_year_data (list): The collected data for all year ranges.\n",
        "    \"\"\"\n",
        "    if not all_year_data:\n",
        "        print(f\"No data to save for topic: {topic}\")\n",
        "        return\n",
        "\n",
        "    # Create a sanitized filename based on the topic name\n",
        "    file_name = f\"{topic.replace(' ', '_')}_data.json\"\n",
        "\n",
        "    # Save the data to a JSON file with proper formatting\n",
        "    with open(file_name, 'w', encoding='utf-8') as f:\n",
        "        json.dump(all_year_data, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(f\"Data saved to {file_name}\")\n",
        "\n",
        "# Define topics and year ranges for scraping\n",
        "topics = [\"Foundation Models\", \"Generative Models\", \"LLM\", \"VLM\", \"Diffusion Models\"]\n",
        "year_ranges = [\"2017-2020\",\"2021-2023\",\"2024-2025\"]\n",
        "\n",
        "# Iterate through each topic and collect research papers\n",
        "for topic in topics:\n",
        "    print(f\"Scraping data for topic: {topic}\")\n",
        "    all_year_data = []\n",
        "\n",
        "    # Scrape papers for each year range\n",
        "    for year_range in year_ranges:\n",
        "        data = scrape_semantic_scholar_api(topic, year_range, limit=1000)\n",
        "        all_year_data.append({'year_range': year_range, 'papers': data})\n",
        "\n",
        "    # Save the collected data to a JSON file\n",
        "    save_to_json_file(topic, all_year_data)\n",
        "    print(f\"Completed scraping for topic: {topic}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGZaKc5MN146",
        "outputId": "a4751acc-440f-4ef1-9e59-8984fae9f80c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraping data for topic: Foundation Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to Foundation_Models_data.json\n",
            "Completed scraping for topic: Foundation Models\n",
            "Scraping data for topic: Generative Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to Generative_Models_data.json\n",
            "Completed scraping for topic: Generative Models\n",
            "Scraping data for topic: LLM\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 825/1000\n",
            "Error: Received 400. Message: {'error': 'Requested data for this limit and/or offset is not available'}.\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 300/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 600/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to LLM_data.json\n",
            "Completed scraping for topic: LLM\n",
            "Scraping data for topic: VLM\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Error: Received 400. Message: {'error': 'Requested data for this limit and/or offset is not available'}.\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 544/1000\n",
            "Error: Received 400. Message: {'error': 'Requested data for this limit and/or offset is not available'}.\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to VLM_data.json\n",
            "Completed scraping for topic: VLM\n",
            "Scraping data for topic: Diffusion Models\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Scraped: 0/1000\n",
            "Scraped: 300/1000\n",
            "Hit rate limit. Pausing for 10 seconds...\n",
            "Scraped: 300/1000\n",
            "Scraped: 600/1000\n",
            "Scraped: 900/1000\n",
            "Data saved to Diffusion_Models_data.json\n",
            "Completed scraping for topic: Diffusion Models\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GGOTtFI-xsi"
      },
      "source": [
        "# Cleaning the dataset and merge:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "\n",
        "# Function to load existing JSON data from a file\n",
        "def load_data_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads JSON data from the given file if it exists.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        list: The loaded JSON data or an empty list if the file is missing or corrupted.\n",
        "    \"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return json.load(file)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: Unable to parse {file_path}. File may be empty or corrupted.\")\n",
        "            return []\n",
        "    else:\n",
        "        print(f\"Warning: {file_path} not found.\")\n",
        "        return []\n",
        "\n",
        "# Function to save data to a JSON file\n",
        "def save_data_to_file(file_path, data):\n",
        "    \"\"\"\n",
        "    Saves the given data to a JSON file with proper formatting.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to save the JSON data.\n",
        "        data (list): The JSON data to store.\n",
        "    \"\"\"\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(data, file, ensure_ascii=False, indent=4)\n",
        "    print(f\"Success: Data saved to {file_path}\")\n",
        "\n",
        "# Function to merge \"papers\" sections from different JSON parts\n",
        "def merge_papers_in_file(file_path):\n",
        "    \"\"\"\n",
        "    Merges the \"papers\" lists from multiple sections in a JSON file into a single list.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): The path to the JSON file containing multiple \"papers\" sections.\n",
        "    \"\"\"\n",
        "    data = load_data_from_file(file_path)\n",
        "\n",
        "    # Ensure there are exactly two sections to merge\n",
        "    if len(data) == 2:\n",
        "        merged_papers = data[0].get('papers', []) + data[1].get('papers', [])\n",
        "        merged_data = [{'papers': merged_papers}]\n",
        "        save_data_to_file(file_path, merged_data)\n",
        "    else:\n",
        "        print(f\"Info: No merge required for {file_path}. Expected 2 sections, found {len(data)}.\")\n",
        "\n",
        "# List of research topic files to process\n",
        "file_names = [\n",
        "    \"Foundation_Models_data.json\",\n",
        "    \"Generative_Models_data.json\",\n",
        "    \"LLM_data.json\",\n",
        "    \"VLM_data.json\",\n",
        "    \"Diffusion_Models_data.json\"\n",
        "]\n",
        "\n",
        "# Process each file and merge paper sections\n",
        "for file_name in file_names:\n",
        "    merge_papers_in_file(file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9nBTrvAO1fy",
        "outputId": "3d78f5be-a7ce-4a15-d3da-e6c3ba868a6b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Info: No merge required for Foundation_Models_data.json. Expected 2 sections, found 3.\n",
            "Info: No merge required for Generative_Models_data.json. Expected 2 sections, found 3.\n",
            "Info: No merge required for LLM_data.json. Expected 2 sections, found 3.\n",
            "Info: No merge required for VLM_data.json. Expected 2 sections, found 3.\n",
            "Info: No merge required for Diffusion_Models_data.json. Expected 2 sections, found 3.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FkBYGt9-xsi"
      },
      "source": [
        "# Visualization:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "import random\n",
        "\n",
        "# Function to load JSON data from a file\n",
        "def load_data_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Loads JSON data from the specified file if it exists.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the JSON file.\n",
        "\n",
        "    Returns:\n",
        "        list: Loaded JSON data or an empty list if the file is missing or corrupted.\n",
        "    \"\"\"\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                return json.load(file)\n",
        "        except json.JSONDecodeError:\n",
        "            print(f\"Error: Could not parse {file_path}. The file may be empty or corrupted.\")\n",
        "            return []\n",
        "    else:\n",
        "        print(f\"Warning: {file_path} not found.\")\n",
        "        return []\n",
        "\n",
        "# Function to display a random paper from the \"papers\" list in the JSON file\n",
        "def show_random_paper_from_file(file_path):\n",
        "    \"\"\"\n",
        "    Displays a randomly selected research paper from the given JSON file.\n",
        "\n",
        "    Parameters:\n",
        "        file_path (str): Path to the JSON file.\n",
        "    \"\"\"\n",
        "    data = load_data_from_file(file_path)\n",
        "\n",
        "    if data and isinstance(data, list) and 'papers' in data[0]:\n",
        "        papers = data[0]['papers']\n",
        "\n",
        "        if papers:\n",
        "            random_paper = random.choice(papers)  # Select a random paper\n",
        "            print(f\"\\nRandom paper from {file_path}:\")\n",
        "            print(f\"Title: {random_paper.get('title', 'No Title')}\")\n",
        "            print(f\"Abstract: {random_paper.get('abstract', 'No Abstract')}\")\n",
        "            print(f\"Authors: {random_paper.get('authors', 'No Authors')}\")\n",
        "            print(f\"Citations: {random_paper.get('citations', 0)}\")\n",
        "            print(\"-\" * 40)\n",
        "        else:\n",
        "            print(f\"Info: No papers found in {file_path}.\")\n",
        "    else:\n",
        "        print(f\"Info: No valid 'papers' key found in {file_path}, or the file is empty.\")\n",
        "\n",
        "# List of research topic files\n",
        "file_names = [\n",
        "    \"Foundation_Models_data.json\",\n",
        "    \"Generative_Models_data.json\",\n",
        "    \"LLM_data.json\",\n",
        "    \"VLM_data.json\",\n",
        "    \"Diffusion_Models_data.json\"\n",
        "]\n",
        "\n",
        "# Display a random paper from each file\n",
        "for file_name in file_names:\n",
        "    show_random_paper_from_file(file_name)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47_g0qxEPGZ5",
        "outputId": "790d5b01-9eac-4c4b-93d9-606ba3fe029b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Random paper from Foundation_Models_data.json:\n",
            "Title: An evaluation of mathematical models for the outbreak of COVID-19\n",
            "Abstract: Abstract Mathematical modelling performs a vital part in estimating and controlling the recent outbreak of coronavirus disease 2019 (COVID-19). In this epidemic, most countries impose severe intervention measures to contain the spread of COVID-19. The policymakers are forced to make difficult decisions to leverage between health and economic development. How and when to make clinical and public health decisions in an epidemic situation is a challenging question. The most appropriate solution is based on scientific evidence, which is mainly dependent on data and models. So one of the most critical problems during this crisis is whether we can develop reliable epidemiological models to forecast the evolution of the virus and estimate the effectiveness of various intervention measures and their impacts on the economy. There are numerous types of mathematical model for epidemiological diseases. In this paper, we present some critical reviews on mathematical models for the outbreak of COVID-19. Some elementary models are presented as an initial formulation for an epidemic. We give some basic concepts, notations, and foundation for epidemiological modelling. More related works are also introduced and evaluated by considering epidemiological features such as disease tendency, latent effects, susceptibility, basic reproduction numbers, asymptomatic infections, herd immunity, and impact of the interventions.\n",
            "Authors: Ning Wang, Yuting Fu, Hu Zhang, Huipeng Shi\n",
            "Citations: 59\n",
            "----------------------------------------\n",
            "\n",
            "Random paper from Generative_Models_data.json:\n",
            "Title: Generative-Discriminative Feature Representations for Open-Set Recognition\n",
            "Abstract: We address the problem of open-set recognition, where the goal is to determine if a given sample belongs to one of the classes used for training a model (known classes). The main challenge in open-set recognition is to disentangle open-set samples that produce high class activations from known-set samples. We propose two techniques to force class activations of open-set samples to be low. First, we train a generative model for all known classes and then augment the input with the representation obtained from the generative model to learn a classifier. This network learns to associate high classification probabilities both when image content is from the correct class as well as when the input and the reconstructed image are consistent with each other. Second, we use self-supervision to force the network to learn more informative featues when assigning class scores to improve separation of classes from each other and from open-set samples. We evaluate the performance of the proposed method with recent open-set recognition works across three datasets, where we obtain state-of-the-art results.\n",
            "Authors: Pramuditha Perera, Vlad I. Morariu, R. Jain, Varun Manjunatha, Curtis Wigington, Vicente Ordonez, Vishal M. Patel\n",
            "Citations: 167\n",
            "----------------------------------------\n",
            "\n",
            "Random paper from LLM_data.json:\n",
            "Title: PERMAINAN INTERAKTIF ELEKTRONIK YANG DIKLASIFIKASIKAN KE DALAM KELOMPOK USIA 18 TAHUN ATAU LEBIH YANG MENGANDUNG HUMOR DEWASA YANG BERKONOTASI SEKSUAL (Analisis Yuridis Pasal 8 huruf d Peraturan Menteri Komunikasi dan Informatika Nomor 11 tahun 2016 tenta\n",
            "Abstract: Ida Bagus Putra Udhyana Pidada , Dr.Bambang Sugiri, SH., MS. Alfons Zakaria, SH, LLM Fakultas Hukum Universitas Brawijaya Email : putraudh@gmail.com Abstrak Peraturan Menteri Komunikasi dan Informatika Nomor 11 Tahun 2016 tentang Klasifikasi Permainan Interaktif Elektronik adalah Peraturan yang bertujuan untuk melindungi kepentingan masyarakat umum dalam mengunakan permainan interaktif elektronik. Diaturnya setiap konten atau muatan yang diperbolehkan dan yang tidak diperbolehkan yang didasarkan oleh pengklasifikasian kelompok usia pengguna yang terbagi menjadi 5 kelompok berdasarkan usia pengguna. Di dalam Pasal 8 Peraturan Menteri tersebut mengatur mengenai konten muatan yang diperbolehkan dalam Kelompok Usia 18 Tahun atau lebih, dimana dalam Pasal 8 huruf d Peraturan Menteri tersebut memperbolehkan adanya konten yang mengandung unsur humor dewasa yang berkonotasi seksual tanpa adanya suatu batasan tertentu, humor dewasa sangatlah luas dan membuka celah hukum untuk masuknya pemainan interaktif elektronik yang memiliki unsur pornografi yang memiliki akibat hukum bertentangan antara peraturan perundang-undangan. Dalam penelitian ini, penuli memilih metode yuridis normatif dengan tujuan untuk menganalisa bahasa hukum tertulis yang mengatur tentang Pornografi dan Informasi Elektronik, sehingga dapat menemukan melalui konsep dari interpretasi gramatikal dari bahasa dan pola hukum tertulis sehingga mampu menemukan jawaban atas konten humor dewasa dengan konotasi seksual dalam permainan interaktif elektronik sebagai suatu bentuk pornografi Kata Kunci : Permainan Interkaktif Elektronik, Pornografi, Humor Dewasa Abstract The Regulation of Minister of Communication and Information Technology number 11 year 2016 about the classification of Electronic Interactive Games is a regulation which protects public interest in using electronic interactive games, it regulates which content is allowed or not based on the classification of the users age which fall into five categories. Article 8 of that regulation states that for the 18-years-old-and-above group, a content containing adult humor with sexual connotation is allowed without any specific limitation. Adult Humor defenition is wide and have a possibilty of the occurence of elements of pornographhy which contradicts the law. This study employed normative juridical method to analyze writtern law regulating Electronic Pornography and Information in order to find the answer to the problems through the concept of grammatical interpretaqtion and writtern law’s pattern. Keywords : Electronic Interactive Game , Pornography, Adult Humour\n",
            "Authors: Ida Bagus Astika Pidada\n",
            "Citations: 0\n",
            "----------------------------------------\n",
            "\n",
            "Random paper from VLM_data.json:\n",
            "Title: Near-IR trigonometric parallaxes of nearby stars in the Galactic plane using the VVV survey\n",
            "Abstract: We use the multi-epoch KS band observations, covering a ∼5 years baseline to obtain milli and sub-milli arcsec precision astrometry for a sample of eighteen previously known high proper motion sources, including precise parallaxes for these sources for the first time. In this pioneer study we show the capability of the VVV project to measure high precision trigonometric parallaxes for very low mass stars (VLMS) up to distances of ∼400 pc reaching farther than most other ground based surveys or space missions for these types of stars. Two stars in our sample are low mass companions to sources in the TGAS catalog, the VVV astrometry of the fainter source is consistent within 1-σ with the astrometry for the primary source in TGAS catalog, confirming the excellent astrometric quality of the VVV data even nearby of saturated sources, as in these cases. Additionally, we used spectral energy distribution to search for evidence of unresolved binary systems and cool subdwarfs. We detected five systems that are most likely VLMS belonging to the Galactic halo based on their tangential velocities, and four objects within 60 pc that are likely members of the thick disk. A more comprehensive study of high proper motion sources and parallaxes of VLMS and brown dwarfs with the VVV is ongoing , including thousands of newly discovered objects (Kurtev et al. 2016).\n",
            "Authors: J. C. Beamín, R. Méndez, R. L. Smart, R. Jara, R. Kurtev, Mariusz Gromadzki, V. Villanueva, D. Minniti, L. Smith, P. W. Lucas\n",
            "Citations: 0\n",
            "----------------------------------------\n",
            "\n",
            "Random paper from Diffusion_Models_data.json:\n",
            "Title: External diffusion of B2B e-procurement and firm financial performance: role of information transparency and supply chain coordination\n",
            "Abstract: PurposeThe purpose of this paper is to understand the process through which external diffusion of business-to-business (B2B) e-procurement impacts firm performance. The research model has been developed to empirically examine the role of information transparency and supply chain coordination in improving the firm financial performance by external diffusion of e-procurement.Design/methodology/approachThe survey is conducted in India with a target population of purchasing professionals working on the B2B e-procurement platform. The measurement model was first tested by using confirmatory factor analysis for reliability and validity, then structural equation modeling (SEM) was used to test the hypotheses of the research model using AMOS 22. The phantom model approach has been used for testing multiple mediators.FindingsThe result of the study highlights the importance of information transparency and supply chain coordination in enhancing the firm financial performance by external diffusion of e-procurement. The results establish the role of information transparency in enhancing firm performance by improving supply chain coordination. The results also indicate that supply chain coordination mediates the relationship between external diffusion of e-procurement and firm financial performance.Originality/valueThis is the first study that has focused on the external diffusion of e-procurement and its impact on firm performance. Also, this study attempted to understand the process through which external diffusion of e-procurement impacts the firm financial performance.\n",
            "Authors: Nripendra Kumar, Kunal K. Ganguly\n",
            "Citations: 29\n",
            "----------------------------------------\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}